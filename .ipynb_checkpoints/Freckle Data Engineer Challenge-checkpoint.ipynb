{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freckle Data Engineer Challenge\n",
    "*22 Oct 2017  \n",
    "Farooq Qaiser* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Freckle_challenge\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some of the basic libraries (we'll load others as we need them). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some display options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "input_path = \"/home/fqaiser94/Data Engineer Challenge/location-data-sample/*.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD vs Dataframe\n",
    "\n",
    "**RE: The expectation for this exercise is that you use Spark 2.x with Scala, Python, or Java. You can use the RDD or Dataframe APIs as you see fit, but please be ready to explain your choices.**  \n",
    "\n",
    "\n",
    "I chose to use the Dataframe API over RDD API because:  \n",
    "1. Dataframe API is able to take advantage of Sparkâ€™s Catalyst optimizer by exposing expressions and data fields to a query planner.\n",
    "2. Dataframe API has speed advantage in most cases (see [here](http://www.adsquare.com/comparing-performance-of-spark-dataframes-api-to-spark-rdd/)). \n",
    "3. I find Dataframes an easier construction to work with  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Always helpful to do some EDA to understand our data before diving in.  \n",
    "Lets take a look at the dataframes schema.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- action: string (nullable = true)\n",
      " |-- api_key: string (nullable = true)\n",
      " |-- app_id: string (nullable = true)\n",
      " |-- beacon_major: long (nullable = true)\n",
      " |-- beacon_minor: long (nullable = true)\n",
      " |-- beacon_uuid: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- community: string (nullable = true)\n",
      " |-- community_code: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- county_code: string (nullable = true)\n",
      " |-- event_time: long (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      " |-- horizontal_accuracy: double (nullable = true)\n",
      " |-- idfa: string (nullable = true)\n",
      " |-- idfa_hash_alg: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- user_ip: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
